{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance Computing - Exercise 1\n",
    "\n",
    "The requirement of this exercise was to study the results given by the OSU MPI benchmark with respect to the latency of communication. Two collective operations were chosen: the *broadcast* and *scatter* operations.\n",
    "\n",
    "Runs of the programs were performed on the ORFEO cluster. In particular, two nodes were selected : EPYC and THIN nodes partitions.\n",
    "\n",
    "### Broadcast operation:\n",
    "\n",
    "The broadcast operation is a MPI standard function that allows the root process to send the same data to all other processes involved. The processes involved are referred to as the communicator group.\n",
    "\n",
    "In order to perform comparisons, three algorithms chosen were the default algorithm given by the benchmark, the chain algorithm and the binary tree algorithm.\n",
    "\n",
    "### Scatter operation:\n",
    "\n",
    "The scatter operation is similar to the broadcast operation except for the fact that data sent to other processes are unique: the root process distributes a portion of the data to each other process inside the communicator group.\n",
    "\n",
    "Again, to perform comparisons the algorithm chosen were the default algorithm, the linear algorithm and linear non-blocking algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Both are blocking operations, which means that none of the proccesses reaching the call of this function can proceed unless the operation is completed.\n",
    "\n",
    "But it is possible to call these same operations as non-blocking functions by using *MPI_Ibcast* and *MPI_Iscatter*. This is done to reduce timing and, thus efficiency, in parallel programming but requires a careful design of the program to correctedness given dependencies between calls inside the program itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis\n",
    "\n",
    "A first glance at results of performance with respect to latency was given by running the program on both nodes to compare the increase in average latency and its standard deviation by changing the number of iterations and warm-up iterations. A single run of the program considers an increasing value of message size and, for each size, it returns the average latency in microseconds computed over a certain number of iterations. The number of warm-up iterations defines the number of iterations the program has to do before starting to record values of latency. This difference helps to ignores values that could be not representative in the analysis, for example because of cache misses or communication initialization.\n",
    "\n",
    "The selected number of iterations and warm-up iterations was based on the minimum average increase in latency for message size that, in these cases, has always corresponded to a minimum in standard deviation as well.\n",
    "\n",
    "After identifying the optimal number of iterations, we selected those values for the algorithms of each operation and compared their results.\n",
    "\n",
    "Results are shown in the following tables.\n",
    "\n",
    "### Broadcast - EPYC node\n",
    "\n",
    "![bcast epyc 0](pre_images/pre_bcast_epyc0.png)\n",
    "\n",
    "![bcast epyc 1](pre_images/pre_bcast_epyc1.png) \n",
    "\n",
    "![bcast epyc 2](pre_images/pre_bcast_epyc2.png)\n",
    "\n",
    "![bcast epyc 3](pre_images/pre_bcast_epyc3.png)\n",
    "\n",
    "### Broadcast - THIN node\n",
    "\n",
    "![bcast thin 0](pre_images/pre_bcast_thin0.png)\n",
    "\n",
    "![bcast thin 1](pre_images/pre_bcast_thin1.png)\n",
    "\n",
    "![bcast thin 2](pre_images/pre_bcast_thin2.png)\n",
    "\n",
    "![bcast thin 3](pre_images/pre_bcast_thin3.png)\n",
    "\n",
    "### Scatter - EPYC node\n",
    "\n",
    "![scatter epyc 0](pre_images/pre_scatter_epyc0.png)\n",
    "\n",
    "![scatter epyc 1](pre_images/pre_scatter_epyc1.png)\n",
    "\n",
    "![scatter epyc 2](pre_images/pre_scatter_epyc2.png)\n",
    "\n",
    "![scatter epyc 3](pre_images/pre_scatter_epyc3.png)\n",
    "\n",
    "### Scatter -THIN node\n",
    "\n",
    "![scatter thin 0](pre_images/pre_scatter_thin0.png)\n",
    "\n",
    "![scatter thin 1](pre_images/pre_scatter_thin1.png)\n",
    "\n",
    "![scatter thin 2](pre_images/pre_scatter_thin2.png)\n",
    "\n",
    "![scatter thin 3](pre_images/pre_scatter_thin3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-to-point communication time analysis\n",
    "\n",
    "Before diving into a deeper analysis of latency for each algorithm considered, we run the osu_latency program and the osu_bw program in order to estimate point-to-point communication time between pairs of cores into a single node. Results were collected in csv files. All csv files were grouped in a single data frame that computed average latency and average bandwidth over all pairs of cores. A third column was added by computing the communication time by means of the following relation:\n",
    "$$\n",
    "T_{comm} = \\lambda + S/B\n",
    "$$\n",
    "where $\\lambda$ is the average latency in seconds, S is the message size in Megabytes, and B is the bandwidth in Megabytes per seconds. \n",
    "\n",
    "### EPYC node\n",
    "\n",
    "![epyc t comm](epyc_tcomm.png)\n",
    "\n",
    "### THIN node\n",
    "\n",
    "![thin t comm](thin_tcomm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability \n",
    "\n",
    "The last analysis was performed by computing the benchmark for each algorithm with an increasing number of cores so as to compare latency over scalability. Furthermore, we compared latency of the algorithms when running using 2 cores with the results of the previous section.\n",
    "\n",
    "### Bcast EPYC node\n",
    "\n",
    "![bcast epyc 0](b_epyc0.png)\n",
    "\n",
    "![bcast epyc 1](b_epyc1.png)\n",
    "\n",
    "### Bcast THIN node\n",
    "\n",
    "![bcast thin 0](b_thin0.png)\n",
    "\n",
    "![bcast thin 1](b_thin1.png)\n",
    "\n",
    "### Scatter EPYC node\n",
    "\n",
    "![scatter epyc 0](s_epyc0.png)\n",
    "\n",
    "![scatter epyc 1](s_epyc1.png)\n",
    "\n",
    "### Scatter THIN node\n",
    "\n",
    "![scatter thin 0](s_thin0.png)\n",
    "\n",
    "![scatter thin 1](s_thin1.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
